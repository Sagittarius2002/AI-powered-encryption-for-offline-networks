{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e41315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8e2a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risky_words</th>\n",
       "      <th>moderate_word</th>\n",
       "      <th>non_risky_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>password</td>\n",
       "      <td>project_plan</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passcode</td>\n",
       "      <td>project_update</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secret</td>\n",
       "      <td>project_outline</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confidential</td>\n",
       "      <td>project_brief</td>\n",
       "      <td>grape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>classified</td>\n",
       "      <td>project_notes</td>\n",
       "      <td>mango</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    risky_words    moderate_word non_risky_words\n",
       "0      password     project_plan           apple\n",
       "1      passcode   project_update          banana\n",
       "2        secret  project_outline          orange\n",
       "3  confidential    project_brief           grape\n",
       "4    classified    project_notes           mango"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc0f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(word, risk_level):\n",
    "    return f\"This document contains the term '{word}', which may indicate {risk_level} risk.\"\n",
    "\n",
    "# Apply to each column\n",
    "df[\"sentence_risky\"] = df[\"risky_words\"].apply(lambda x: generate_sentence(x, \"high\"))\n",
    "df[\"sentence_moderate\"] = df[\"moderate_word\"].apply(lambda x: generate_sentence(x, \"moderate\"))\n",
    "df[\"sentence_low\"] = df[\"non_risky_words\"].apply(lambda x: generate_sentence(x, \"low\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ccea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  This document contains the term 'customer_segm...      0\n",
      "1  This document contains the term 'db_service', ...      0\n",
      "2  This document contains the term 'policy_handbo...      1\n",
      "3  This document contains the term 'pickle', whic...      2\n",
      "4  This document contains the term 'taco', which ...      2\n"
     ]
    }
   ],
   "source": [
    "# Create labeled datasets\n",
    "risky_df = pd.DataFrame({\n",
    "    \"text\": df[\"sentence_risky\"],\n",
    "    \"label\": [0] * len(df)  # High Risk = 0\n",
    "})\n",
    "\n",
    "moderate_df = pd.DataFrame({\n",
    "    \"text\": df[\"sentence_moderate\"],\n",
    "    \"label\": [1] * len(df)  # Moderate Risk = 1\n",
    "})\n",
    "\n",
    "low_df = pd.DataFrame({\n",
    "    \"text\": df[\"sentence_low\"],\n",
    "    \"label\": [2] * len(df)  # Low Risk = 2\n",
    "})\n",
    "\n",
    "# Combine all\n",
    "full_df = pd.concat([risky_df, moderate_df, low_df], ignore_index=True)\n",
    "\n",
    "# Shuffle for training\n",
    "full_df = full_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f69d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9742aad70fda4a0786db71b1350765d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2323f725c04683954c0bb0f10215ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f94574b6ef461796fe766089e7d373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5652853e72f496b834fc4ebf2fdb7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646bdcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text column\n",
    "encodings = tokenizer(\n",
    "    full_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=64,  # You can adjust this based on sentence length\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78cad1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor(full_df[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0392b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Combine into a PyTorch dataset\n",
    "dataset = TensorDataset(\n",
    "    encodings[\"input_ids\"],\n",
    "    encodings[\"attention_mask\"],\n",
    "    labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3949f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a495e3c61b484f28bf7782e659ee85e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Load model with 3 output labels (High, Moderate, Low Risk)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c37176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 80/20 split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0154fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.17.0\n",
      "transformers.training_args\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ef6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments created successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "febea7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c55f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "def tuple_to_dict_collator(features):\n",
    "    input_ids, attention_mask, labels = zip(*features)\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids),\n",
    "        \"attention_mask\": torch.stack(attention_mask),\n",
    "        \"labels\": torch.stack(labels)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=tuple_to_dict_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d988cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sayan\\.conda\\envs\\myenv\\Lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2700\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b0ee2b38e845fe992bffed59053387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0978, 'learning_rate': 4.85207100591716e-05, 'epoch': 0.03}\n",
      "{'loss': 0.9298, 'learning_rate': 4.7041420118343196e-05, 'epoch': 0.06}\n",
      "{'loss': 0.5561, 'learning_rate': 4.556213017751479e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1352, 'learning_rate': 4.408284023668639e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0321, 'learning_rate': 4.260355029585799e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0129, 'learning_rate': 4.112426035502959e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0075, 'learning_rate': 3.964497041420119e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0055, 'learning_rate': 3.8165680473372784e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0045, 'learning_rate': 3.668639053254438e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0036, 'learning_rate': 3.520710059171598e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0031, 'learning_rate': 3.3727810650887574e-05, 'epoch': 0.33}\n",
      "{'loss': 0.003, 'learning_rate': 3.224852071005917e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0025, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0022, 'learning_rate': 2.9289940828402368e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0022, 'learning_rate': 2.7810650887573965e-05, 'epoch': 0.44}\n",
      "{'loss': 0.002, 'learning_rate': 2.6331360946745565e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0019, 'learning_rate': 2.485207100591716e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0016, 'learning_rate': 2.337278106508876e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0017, 'learning_rate': 2.189349112426036e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0015, 'learning_rate': 2.0414201183431952e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0014, 'learning_rate': 1.8934911242603552e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0014, 'learning_rate': 1.745562130177515e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0015, 'learning_rate': 1.5976331360946746e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0013, 'learning_rate': 1.4497041420118343e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0012, 'learning_rate': 1.3017751479289941e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0012, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0012, 'learning_rate': 1.0059171597633136e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0011, 'learning_rate': 8.579881656804733e-06, 'epoch': 0.83}\n",
      "{'loss': 0.0011, 'learning_rate': 7.100591715976332e-06, 'epoch': 0.86}\n",
      "{'loss': 0.0012, 'learning_rate': 5.621301775147929e-06, 'epoch': 0.89}\n",
      "{'loss': 0.0012, 'learning_rate': 4.142011834319527e-06, 'epoch': 0.92}\n",
      "{'loss': 0.0011, 'learning_rate': 2.6627218934911246e-06, 'epoch': 0.95}\n",
      "{'loss': 0.0011, 'learning_rate': 1.183431952662722e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2c3afdde014b1593dcc54f5ebe076d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./test_model\\checkpoint-338\n",
      "Configuration saved in ./test_model\\checkpoint-338\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000771350518334657, 'eval_accuracy': 1.0, 'eval_f1_macro': 1.0, 'eval_runtime': 1.1965, 'eval_samples_per_second': 564.165, 'eval_steps_per_second': 71.043, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./test_model\\checkpoint-338\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./test_model\\checkpoint-338 (score: 1.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 33.6705, 'train_samples_per_second': 80.189, 'train_steps_per_second': 10.038, 'train_loss': 0.08354047423548247, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=338, training_loss=0.08354047423548247, metrics={'train_runtime': 33.6705, 'train_samples_per_second': 80.189, 'train_steps_per_second': 10.038, 'train_loss': 0.08354047423548247, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f77d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d86a3c2b9141538513591f4a52982f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation: {'eval_loss': 0.000771350518334657, 'eval_accuracy': 1.0, 'eval_f1_macro': 1.0, 'eval_runtime': 1.2479, 'eval_samples_per_second': 540.893, 'eval_steps_per_second': 68.112, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"Final Evaluation:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "900a9093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423175107b7843468a2dd07cc04cb436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[225   0   0]\n",
      " [  0 203   0]\n",
      " [  0   0 247]]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# Compare with true labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d1ee970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress Test Evaluation: {'eval_loss': 0.000859028659760952, 'eval_accuracy': 1.0, 'eval_f1_macro': 1.0, 'eval_runtime': 1.095, 'eval_samples_per_second': 616.426, 'eval_steps_per_second': 77.624, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Alternate templates\n",
    "templates = [\n",
    "    \"The presence of '{word}' in this file suggests {risk} risk.\",\n",
    "    \"Detected term: '{word}' â€” classified as {risk} risk.\",\n",
    "    \"This text contains '{word}', indicating a {risk} risk category.\",\n",
    "    \"We found '{word}' here, which could mean {risk} risk.\",\n",
    "    \"'{word}' appears in the document, suggesting {risk} risk.\"\n",
    "]\n",
    "\n",
    "# Simple noise words\n",
    "noise_words = [\"please\", \"urgent\", \"note\", \"xyz123\", \"check\", \"randomword\"]\n",
    "\n",
    "# Risk label mapping\n",
    "label_map = {0: \"high\", 1: \"moderate\", 2: \"low\"}\n",
    "\n",
    "def stress_sentence(word, risk):\n",
    "    # Pick a random template\n",
    "    sentence = random.choice(templates).format(word=word, risk=risk)\n",
    "    # Randomly inject noise\n",
    "    if random.random() < 0.5:\n",
    "        insert_pos = random.randint(0, len(sentence.split()))\n",
    "        words = sentence.split()\n",
    "        words.insert(insert_pos, random.choice(noise_words))\n",
    "        sentence = \" \".join(words)\n",
    "    return sentence\n",
    "\n",
    "# Build stress test dataset from original test set\n",
    "stress_texts = []\n",
    "stress_labels = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    # Extract original tuple from TensorDataset\n",
    "    input_ids, attention_mask, label = test_dataset[i]\n",
    "    # Get the original word from your full_df (optional: store mapping earlier)\n",
    "    # For now, we'll just simulate with placeholder words\n",
    "    word = f\"term{i}\"  # Replace with actual mapping if available\n",
    "    risk_str = label_map[int(label)]\n",
    "    stress_texts.append(stress_sentence(word, risk_str))\n",
    "    stress_labels.append(int(label))\n",
    "\n",
    "# Tokenize stress test set\n",
    "stress_encodings = tokenizer(\n",
    "    stress_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=64,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "stress_dataset = TensorDataset(\n",
    "    stress_encodings[\"input_ids\"],\n",
    "    stress_encodings[\"attention_mask\"],\n",
    "    torch.tensor(stress_labels)\n",
    ")\n",
    "\n",
    "# Evaluate on stress test\n",
    "stress_results = trainer.evaluate(stress_dataset)\n",
    "print(\"Stress Test Evaluation:\", stress_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3ddc252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./distilbert_risk_model\\config.json\n",
      "Model weights saved in ./distilbert_risk_model\\pytorch_model.bin\n",
      "tokenizer config file saved in ./distilbert_risk_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./distilbert_risk_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./distilbert_risk_model\\\\tokenizer_config.json',\n",
       " './distilbert_risk_model\\\\special_tokens_map.json',\n",
       " './distilbert_risk_model\\\\vocab.txt',\n",
       " './distilbert_risk_model\\\\added_tokens.json',\n",
       " './distilbert_risk_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./distilbert_risk_model\")\n",
    "tokenizer.save_pretrained(\"./distilbert_risk_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d1e271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./distilbert_risk_model\\added_tokens.json. We won't load it.\n",
      "loading file ./distilbert_risk_model\\vocab.txt\n",
      "loading file ./distilbert_risk_model\\tokenizer.json\n",
      "loading file None\n",
      "loading file ./distilbert_risk_model\\special_tokens_map.json\n",
      "loading file ./distilbert_risk_model\\tokenizer_config.json\n",
      "loading configuration file ./distilbert_risk_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./distilbert_risk_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./distilbert_risk_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Risk\n",
      "High Risk\n",
      "Moderate Risk\n",
      "High Risk\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"./distilbert_risk_model\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./distilbert_risk_model\")\n",
    "\n",
    "# Map numeric labels back to human-readable classes\n",
    "label_map = {0: \"High Risk\", 1: \"Moderate Risk\", 2: \"Low Risk\"}\n",
    "\n",
    "def classify_text(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    \n",
    "    # Run inference without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_class_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return label_map[predicted_class_id]\n",
    "\n",
    "# Test with any sentence\n",
    "print(classify_text(\"I am Sayan Ghosh\"))\n",
    "print(classify_text(\"Confidential: Military Files\"))\n",
    "print(classify_text(\"The presence of outdated encryption protocols in this system may indicate a moderate risk to data security.\"))\n",
    "print(classify_text(\"This document contains the term 'butcher'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c088a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
