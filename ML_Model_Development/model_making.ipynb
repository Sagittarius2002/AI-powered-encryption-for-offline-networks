{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e41315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8e2a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risky_words</th>\n",
       "      <th>moderate_word</th>\n",
       "      <th>non_risky_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>password</td>\n",
       "      <td>project_plan</td>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passcode</td>\n",
       "      <td>project_update</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secret</td>\n",
       "      <td>project_outline</td>\n",
       "      <td>orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>confidential</td>\n",
       "      <td>project_brief</td>\n",
       "      <td>grape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>classified</td>\n",
       "      <td>project_notes</td>\n",
       "      <td>mango</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    risky_words    moderate_word non_risky_words\n",
       "0      password     project_plan           apple\n",
       "1      passcode   project_update          banana\n",
       "2        secret  project_outline          orange\n",
       "3  confidential    project_brief           grape\n",
       "4    classified    project_notes           mango"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d62f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_templates = [\n",
    "    \"Critical alert: '{word}' detected — immediate action required.\",\n",
    "    \"Severe security breach involving '{word}' found in the system.\",\n",
    "    \"Evidence of '{word}' poses a serious threat to operations.\",\n",
    "    \"Urgent: '{word}' could lead to catastrophic data loss.\",\n",
    "    \"'{word}' identified — potential for major financial and reputational damage.\",\n",
    "    \"High‑priority incident: '{word}' may compromise core infrastructure.\",\n",
    "    \"Detected '{word}' — situation classified as critical.\",\n",
    "    \"Security systems report '{word}' with potential for widespread impact.\",\n",
    "    \"Immediate containment advised — '{word}' present in sensitive modules.\",\n",
    "    \"System integrity at risk due to '{word}' occurrence.\"\n",
    "]\n",
    "\n",
    "moderate_templates = [\n",
    "    \"Warning: '{word}' may cause issues if left unresolved.\",\n",
    "    \"The use of '{word}' could lead to performance or compliance concerns.\",\n",
    "    \"Caution: '{word}' might introduce vulnerabilities over time.\",\n",
    "    \"Review recommended — '{word}' may not meet current best practices.\",\n",
    "    \"'{word}' detected — requires monitoring and possible mitigation.\",\n",
    "    \"Potential issue: '{word}' could affect non‑critical systems.\",\n",
    "    \"Detected '{word}' — advisable to schedule a security review.\",\n",
    "    \"Operational note: '{word}' may degrade efficiency if ignored.\",\n",
    "    \"Audit finding: '{word}' should be addressed in upcoming maintenance.\",\n",
    "    \"Minor irregularity involving '{word}' — attention suggested.\"\n",
    "]\n",
    "\n",
    "low_templates = [\n",
    "    \"Note: '{word}' is present but poses no immediate concern.\",\n",
    "    \"Informational: '{word}' found — considered safe under current conditions.\",\n",
    "    \"'{word}' detected — no action needed at this time.\",\n",
    "    \"Routine check: '{word}' is compliant with standards.\",\n",
    "    \"'{word}' appears in the document — harmless in current context.\",\n",
    "    \"Detected '{word}' — no operational impact expected.\",\n",
    "    \"System log shows '{word}' — within acceptable parameters.\",\n",
    "    \"Observation: '{word}' is part of normal configuration.\",\n",
    "    \"No threat detected from '{word}' in current environment.\",\n",
    "    \"Standard usage of '{word}' confirmed — no intervention required.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab991a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  'table_runner' appears in the document — harml...      2\n",
      "1  Caution: 'unit_reference_summary' might introd...      1\n",
      "2  Operational note: 'project_update' may degrade...      1\n",
      "3  Severe security breach involving 'db_backup_ke...      0\n",
      "4  Note: 'stem' is present but poses no immediate...      2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(word, templates):\n",
    "    return random.choice(templates).format(word=word)\n",
    "\n",
    "risky_df = pd.DataFrame({\n",
    "    \"text\": df[\"risky_words\"].apply(lambda x: generate_sentence(x, high_templates)),\n",
    "    \"label\": [0] * len(df)\n",
    "})\n",
    "\n",
    "moderate_df = pd.DataFrame({\n",
    "    \"text\": df[\"moderate_word\"].apply(lambda x: generate_sentence(x, moderate_templates)),\n",
    "    \"label\": [1] * len(df)\n",
    "})\n",
    "\n",
    "low_df = pd.DataFrame({\n",
    "    \"text\": df[\"non_risky_words\"].apply(lambda x: generate_sentence(x, low_templates)),\n",
    "    \"label\": [2] * len(df)\n",
    "})\n",
    "\n",
    "# Combine all\n",
    "full_df = pd.concat([risky_df, moderate_df, low_df], ignore_index=True)\n",
    "\n",
    "# Shuffle for training\n",
    "full_df = full_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Preview\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f69d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\", local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646bdcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text column\n",
    "encodings = tokenizer(\n",
    "    full_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=64,  # You can adjust this based on sentence length\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78cad1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "labels = torch.tensor(full_df[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0392b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Combine into a PyTorch dataset\n",
    "dataset = TensorDataset(\n",
    "    encodings[\"input_ids\"],\n",
    "    encodings[\"attention_mask\"],\n",
    "    labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b3949f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Load model with 3 output labels (High, Moderate, Low Risk)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    local_files_only=True,\n",
    "    num_labels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c37176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# 80/20 split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0154fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.17.0\n",
      "transformers.training_args\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ef6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments created successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febea7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c55f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "def tuple_to_dict_collator(features):\n",
    "    input_ids, attention_mask, labels = zip(*features)\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids),\n",
    "        \"attention_mask\": torch.stack(attention_mask),\n",
    "        \"labels\": torch.stack(labels)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=tuple_to_dict_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d988cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sayan\\.conda\\envs\\myenv\\Lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2700\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cd0adc421b4417a4583707c567393a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0413, 'learning_rate': 4.85207100591716e-05, 'epoch': 0.03}\n",
      "{'loss': 0.667, 'learning_rate': 4.7041420118343196e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2767, 'learning_rate': 4.556213017751479e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0779, 'learning_rate': 4.408284023668639e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0233, 'learning_rate': 4.260355029585799e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0115, 'learning_rate': 4.112426035502959e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0068, 'learning_rate': 3.964497041420119e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0053, 'learning_rate': 3.8165680473372784e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0041, 'learning_rate': 3.668639053254438e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0036, 'learning_rate': 3.520710059171598e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0032, 'learning_rate': 3.3727810650887574e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0029, 'learning_rate': 3.224852071005917e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0024, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0021, 'learning_rate': 2.9289940828402368e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0022, 'learning_rate': 2.7810650887573965e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0019, 'learning_rate': 2.6331360946745565e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0019, 'learning_rate': 2.485207100591716e-05, 'epoch': 0.5}\n",
      "{'loss': 0.0017, 'learning_rate': 2.337278106508876e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0016, 'learning_rate': 2.189349112426036e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0015, 'learning_rate': 2.0414201183431952e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0015, 'learning_rate': 1.8934911242603552e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0013, 'learning_rate': 1.745562130177515e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0014, 'learning_rate': 1.5976331360946746e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0012, 'learning_rate': 1.4497041420118343e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0012, 'learning_rate': 1.3017751479289941e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0011, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0012, 'learning_rate': 1.0059171597633136e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0011, 'learning_rate': 8.579881656804733e-06, 'epoch': 0.83}\n",
      "{'loss': 0.0011, 'learning_rate': 7.100591715976332e-06, 'epoch': 0.86}\n",
      "{'loss': 0.0011, 'learning_rate': 5.621301775147929e-06, 'epoch': 0.89}\n",
      "{'loss': 0.0011, 'learning_rate': 4.142011834319527e-06, 'epoch': 0.92}\n",
      "{'loss': 0.001, 'learning_rate': 2.6627218934911246e-06, 'epoch': 0.95}\n",
      "{'loss': 0.001, 'learning_rate': 1.183431952662722e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413f07aa60c948f7b11b08975a8e5bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./test_model\\checkpoint-338\n",
      "Configuration saved in ./test_model\\checkpoint-338\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007498149643652141, 'eval_accuracy': 1.0, 'eval_f1_macro': 1.0, 'eval_runtime': 1.0141, 'eval_samples_per_second': 665.643, 'eval_steps_per_second': 83.822, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./test_model\\checkpoint-338\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./test_model\\checkpoint-338 (score: 1.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 31.546, 'train_samples_per_second': 85.589, 'train_steps_per_second': 10.715, 'train_loss': 0.063754687129834, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=338, training_loss=0.063754687129834, metrics={'train_runtime': 31.546, 'train_samples_per_second': 85.589, 'train_steps_per_second': 10.715, 'train_loss': 0.063754687129834, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f77d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bba49787f3644789a7398f6c203998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation: {'eval_loss': 0.0007498149643652141, 'eval_accuracy': 1.0, 'eval_f1_macro': 1.0, 'eval_runtime': 1.1145, 'eval_samples_per_second': 605.655, 'eval_steps_per_second': 76.268, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"Final Evaluation:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900a9093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a95257ebf943749cf2ed7d41ee743a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[213   0   0]\n",
      " [  0 240   0]\n",
      " [  0   0 222]]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# Compare with true labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d1ee970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stress Test Evaluation: {'eval_loss': 4.38728141784668, 'eval_accuracy': 0.36, 'eval_f1_macro': 0.23266647161283785, 'eval_runtime': 1.2971, 'eval_samples_per_second': 520.391, 'eval_steps_per_second': 65.531, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Alternate templates\n",
    "templates = [\n",
    "    \"Detected '{word}' — advisable to schedule a security review.\",\n",
    "\"Operational note: '{word}' may degrade efficiency if ignored.\",\n",
    "\"Observation: '{word}' could lead to minor performance issues.\",\n",
    "\"Audit finding: '{word}' should be addressed in upcoming maintenance.\",\n",
    "\"System check: '{word}' may require configuration adjustments.\",\n",
    "\"Report indicates '{word}' could cause intermittent disruptions.\",\n",
    "\"Inspection shows '{word}' present — monitor for changes.\",\n",
    "\"Detected '{word}' — recommend follow‑up in next review cycle.\",\n",
    "\"Maintenance note: '{word}' might impact secondary processes.\",\n",
    "\"Analysis suggests '{word}' could introduce avoidable overhead.\"\n",
    "]\n",
    "\n",
    "# Simple noise words\n",
    "noise_words = [\"please\", \"urgent\", \"note\", \"xyz123\", \"check\", \"randomword\"]\n",
    "\n",
    "# Risk label mapping\n",
    "label_map = {0: \"high\", 1: \"moderate\", 2: \"low\"}\n",
    "\n",
    "def stress_sentence(word):\n",
    "    # Pick a random template\n",
    "    sentence = random.choice(templates).format(word=word)\n",
    "    # Randomly inject noise\n",
    "    if random.random() < 0.5:\n",
    "        insert_pos = random.randint(0, len(sentence.split()))\n",
    "        words = sentence.split()\n",
    "        words.insert(insert_pos, random.choice(noise_words))\n",
    "        sentence = \" \".join(words)\n",
    "    return sentence\n",
    "\n",
    "# Build stress test dataset from original test set\n",
    "stress_texts = []\n",
    "stress_labels = []\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    # Extract original tuple from TensorDataset\n",
    "    input_ids, attention_mask, label = test_dataset[i]\n",
    "    # Get the original word from your full_df (optional: store mapping earlier)\n",
    "    # For now, we'll just simulate with placeholder words\n",
    "    word = f\"term{i}\"  # Replace with actual mapping if available\n",
    "    # risk_str = label_map[int(label)]\n",
    "    stress_texts.append(stress_sentence(word))\n",
    "    stress_labels.append(int(label))\n",
    "\n",
    "# Tokenize stress test set\n",
    "stress_encodings = tokenizer(\n",
    "    stress_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=64,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "stress_dataset = TensorDataset(\n",
    "    stress_encodings[\"input_ids\"],\n",
    "    stress_encodings[\"attention_mask\"],\n",
    "    torch.tensor(stress_labels)\n",
    ")\n",
    "\n",
    "# Evaluate on stress test\n",
    "stress_results = trainer.evaluate(stress_dataset)\n",
    "print(\"Stress Test Evaluation:\", stress_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ddc252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./distilbert_risk_model\\config.json\n",
      "Model weights saved in ./distilbert_risk_model\\pytorch_model.bin\n",
      "tokenizer config file saved in ./distilbert_risk_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./distilbert_risk_model\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./distilbert_risk_model\\\\tokenizer_config.json',\n",
       " './distilbert_risk_model\\\\special_tokens_map.json',\n",
       " './distilbert_risk_model\\\\vocab.txt',\n",
       " './distilbert_risk_model\\\\added_tokens.json',\n",
       " './distilbert_risk_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./distilbert_risk_model\")\n",
    "tokenizer.save_pretrained(\"./distilbert_risk_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d1e271f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./distilbert_risk_model\\added_tokens.json. We won't load it.\n",
      "loading file ./distilbert_risk_model\\vocab.txt\n",
      "loading file ./distilbert_risk_model\\tokenizer.json\n",
      "loading file None\n",
      "loading file ./distilbert_risk_model\\special_tokens_map.json\n",
      "loading file ./distilbert_risk_model\\tokenizer_config.json\n",
      "loading configuration file ./distilbert_risk_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./distilbert_risk_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./distilbert_risk_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Risk\n",
      "High Risk\n",
      "Moderate Risk\n",
      "Moderate Risk\n",
      "Low Risk\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"./distilbert_risk_model\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"./distilbert_risk_model\")\n",
    "\n",
    "# Map numeric labels back to human-readable classes\n",
    "label_map = {0: \"High Risk\", 1: \"Moderate Risk\", 2: \"Low Risk\"}\n",
    "\n",
    "def classify_text(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=64)\n",
    "    \n",
    "    # Run inference without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_class_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return label_map[predicted_class_id]\n",
    "\n",
    "# Test with any sentence\n",
    "print(classify_text(\"I am Sayan Ghosh\"))\n",
    "print(classify_text(\"Confidential: Military Files\"))\n",
    "print(classify_text(\"Backdoor detected, requires monitoring and possible mitigation\"))\n",
    "print(classify_text(\"Potential issue: not required much attention\"))\n",
    "print(classify_text(\"department_plan is next week\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c34c088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4a72a4a4184ababb3f430805c3b566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted classes: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = preds.predictions.argmax(axis=1)\n",
    "print(\"Unique predicted classes:\", set(pred_labels.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c9dba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 675\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 240, 2: 222, 0: 213})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = preds.predictions.argmax(axis=1)\n",
    "print(Counter(pred_labels.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eb005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
